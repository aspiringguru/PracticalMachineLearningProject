setwd("G:/2015/coursera/data_science/practical_machine_learning/project")

## reminder to insert code to download files 
## https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
## https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## load required libraries
## check if all these libraries are actually needed.
library(caret)  ##NB: loads lattice & ggplot2
##library(rpart)
##library(rpart.plot)
##library(randomForest)
##library(corrplot)

## load data.
testBulk <- read.csv("pml-testing.csv")
trainBulk <- read.csv("pml-training.csv")

dim(trainBulk) ##[1] 19622   160
dim(testBulk)  ##[1]  20 160

## remove columns not useful for prediction
removeIndex <- as.integer(c(1:7))
names(trainBulk[,1:7])
names(testBulk[,1:7])
trainBulk <- trainBulk[,-removeIndex]
testBulk <- testBulk[,-removeIndex]
dim(trainBulk) ##[1] 19622   153
dim(testBulk)  ##[1]  20 153


## nearZeroVar {caret}
## nearZeroVar diagnoses predictors that have one unique value (i.e. are zero variance predictors) 
## or predictors that are have both of the following characteristics: 
## - they have very few unique values relative to the number of samples 
## - the ratio of the frequency of the most common value to the frequency of the second most common value is large.
trainBulk_nzv <- nearZeroVar(trainBulk, saveMetrics=TRUE)
trainBulk_remaining <- trainBulk_nzv[which(trainBulk_nzv$nzv==FALSE),]
trainBulk <- subset(trainBulk , select=rownames(trainBulk_remaining))
dim(trainBulk) ## [1] 19622    94 ##153-94=59 columns removed due to lack of variance.

##clean data - remove columns with NA values.
NAs <- apply(trainBulk,2,function(x) {sum(is.na(x))}) 
## apply {base}, Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.
## apply(matrix_to_evaluate, 1=over-rows|2=over-columns, function-to-apply)
##NAs = vector with zero for columns with nil NA values, use NAs to keep these columns only.
cleanTrain <- trainBulk[,which(NAs == 0)]
cleanTest <- testBulk[,which(NAs == 0)]

dim(cleanTrain) ##[1] 19622    53
dim(cleanTest)  ##[1] 20 53

## build data sets for training and cross validation from the cleanTrain data set. (pml-training.csv)
## NB : testing data set (cleanTest) is from pml-testing.csv
trainIndex <- createDataPartition(y = cleanTrain$classe, p=0.7,list=FALSE)
trainSet <- cleanTrain[trainIndex,]
crossValidationSet <- cleanTrain[-trainIndex,]
dim(trainSet)           ## [1] 13737    53
dim(crossValidationSet) ## [1] 5885   53


## refer lectures wk2-1 caret package slide 6-7/11 (gml model example)
modelFit <- train(classe ~ ., data = trainSet, method="glm")
## error message. this is odd. generalised linear model should work. trainSet should have adequate data.
dim(trainSet) ##[1] 13737    53
names(trainSet) ##
------------------------------------------------------ 
Something is wrong; all the Accuracy metric values are missing:
    Accuracy       Kappa    
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
warnings()
Warning messages:
1: In eval(expr, envir, enclos) :
  model fit failed for Resample01: parameter=none Error in method$fit(x = x, y = y, wts = wts, param = tuneValue, lev = obsLevels,  : 
  glm models can only use 2-class outcomes
  repeated n times
  glm models can only use 2-class outcomes
----------------------------------------------------------
mytrControl = trainControl(method = "cv", number = 4)
modelFit <- train(trainSet$classe ~.,data = trainSet, method="rf", trControl = mytrControl)
## this took > 30' to run.
modelFit
---------------------------------------------
Random Forest 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (4 fold) 

Summary of sample sizes: 10304, 10304, 10301, 10302 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9910462  0.9886726  0.003009365  0.003806771
  27    0.9907551  0.9883054  0.002639478  0.003338281
  52    0.9839855  0.9797392  0.002476140  0.003132799

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
----------------------------------------------
modelFit$finalModel

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 0.6%
Confusion matrix:
     A    B    C    D    E  class.error
A 3903    2    0    0    1 0.0007680492
B   14 2639    5    0    0 0.0071482318
C    0   19 2376    1    0 0.0083472454
D    0    0   34 2216    2 0.0159857904
E    0    0    0    5 2520 0.0019801980
-----------------------------------------------------
